{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9pvsfQOHEZx"
   },
   "source": [
    "<h1><center><b>Project 1: Sequence Labeling</b></center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Introduction\n",
    "In this project we explore sequence labeling problems in NLP: (i) *Part of Speech (POS) tagging;* and (ii) *Named Entity Recognition* (NER). We first use the Hidden Markov Model (HMM) to resolve the problem of POS tagging. We then apply Conditional Random Fields (CRF) to find solutions for NER.\n",
    "\n",
    "We uncover some important aspects for NER that require external (human) intervention, for example, the choice of features.\n",
    "\n",
    "* A good read for this project could be **Chapter 17: Sequence Labeling for Parts of Speech and Named Entities of Jurafsky & Martin's Speech and Language Processing book (SLP3)**.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "* Part 0 sets up the environment through installation of the required dependencies and incorporates a kernel restart once the installation finishes.\n",
    "* Part 1 downloads and explores the dataset.\n",
    "* Part 2 investigates the application of Viterbi to Part of Speech tagging.\n",
    "* Part 3 develops a CRF model for NER tasks.\n",
    "* Part 4 provides instructions for uploading responses and information about grading.\n",
    "\n",
    "Each of the major parts of this project includes both programming questions (in Gradescope) and quiz questions (in Canvas) that must be answered in order to receive full credit for this assignment.\n",
    "\n",
    "\n",
    "**Important note on grading**:\n",
    "1. **Project 1 Programming questions** (in Gradescope) - Some of the functions are incomplete; complete them and pass the associated test cases. Passing a test case does not guarantee full credit. Update your answers in the .py file provided to you and upload it to Gradescope for grading.\n",
    "\n",
    "2. **Project 1 Quiz questions** (in Canvas) - This project is accompanied by a project quiz. For each project quiz question, select one or more correct options and explain the response in 200 characters or less. Any quiz response that does not include an explanation will not be graded. Explanations must be no longer than 200 characters; any additional words will result in automatic truncation and will not be considered for grading.\n",
    "\n",
    "**Review Materials for Project 1: Weeks 3-7**\n",
    "* Videos/handouts\n",
    "* FTF slides\n",
    "* Readings per the syllabus: SLP 3, 17, 18, 19, 13.5 (\"chunking\" from SLP edition 2), Universal Dependencies, LDC Penn Treebank, NLTK 0, 2, 7, Scikit Learn, CRFSuite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzLLUuyG1kT7"
   },
   "source": [
    "# Part 0: Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph_DqteCd2Zm"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lucqqa3dIbr_"
   },
   "source": [
    "## 0.1 Running cells\n",
    "\n",
    "You can run the code in a cell by pressing (CMD/CTRL + ENTER) or by clicking the run button in the top left corner.\n",
    "\n",
    "Cells are divided into two categories:\n",
    "1. Markdown - This is a Markdown cell, which is used to provide instructions, similar to a running readme of your code.\n",
    "2. Code cell - The Code cell contains all the code.\n",
    "\n",
    "**Important Points:**\n",
    "\n",
    "**Jupyter Notebook cells are executed in order - from top to bottom. Any other order is not guaranteed to produce expected results.**\n",
    "\n",
    "**You must run all the cells in this notebook. (Even if you are not editing them)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku9pRPOhq3lR"
   },
   "source": [
    "## 0.2 Install Dependencies.\n",
    "\n",
    "The dependencies listed below help us in developing NLP solutions without having to write a lot of code from scratch.\n",
    "\n",
    "The compatible version of scikit-learn package with sklearn-crfsuite is less than 24.0, as higher versions may have Attribute_Errors.\n",
    "\n",
    "Please feel free to learn more about the scikit-learn: https://scikit-learn.org/stable/ and sklearn_crfsuite: https://sklearn-crfsuite.readthedocs.io/en/latest/\n",
    "\n",
    "**Run the code cell below to install required dependencies.**\n",
    "\n",
    "> After installation, you may need to restart the kernel. This can be done by selecting `Restart runtime` in the `Runtime` drop-down menu. But here for your convenience, the following code cell is automatically prompted to restart the kernel after installation is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUD0wRIoZL2v"
   },
   "source": [
    "### If you use **google colab**, keep these lines below (lines 3-4) uncommented\n",
    "### If you use **hipergator**, comment out these lines below (lines 3-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "Yz1h_m_bnuze"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /home/carson.johnson/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sklearn_crfsuite\n",
      "  Cloning https://github.com/MeMartijn/updated-sklearn-crfsuite.git to /scratch/local/46902106/pip-install-2dwae1fq/sklearn-crfsuite_1ad65d5ef99a4ad893d3d6167e40d6e6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MeMartijn/updated-sklearn-crfsuite.git /scratch/local/46902106/pip-install-2dwae1fq/sklearn-crfsuite_1ad65d5ef99a4ad893d3d6167e40d6e6\n",
      "  Resolved https://github.com/MeMartijn/updated-sklearn-crfsuite.git to commit 675038761b4405f04691a83339d04903790e2b95\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from sklearn_crfsuite) (4.66.1)\n",
      "Requirement already satisfied: six in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tabulate in /apps/jupyter/6.5.4/lib/python3.10/site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/carson.johnson/.local/lib/python3.10/site-packages (from sklearn_crfsuite) (0.9.11)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -angchain (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entence-transformers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -idgetsnbextension (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nstructured (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/apps/jupyter/6.5.4/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Run this cell to install required dependencies.\n",
    "\n",
    "!pip install -U scikit-learn\n",
    "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9gZlrtBsFfR"
   },
   "source": [
    "# Part 1: Getting Familiar with the Dataset\n",
    "\n",
    "**NLTK:** There are many datasets in **Natural Language Toolkit (NLTK)** library of Python (see https://www.nltk.org). The two most widely used existing NLTK corpora are:\n",
    "\n",
    "1. Penn TreeBank Corpus\n",
    "2. CoNLL Named Entity (NE) Chunk Corpus\n",
    "\n",
    "We import the NLTK library to access these built-in datasets. To learn more about these and other corpora, see **NLTK Chapter 2: Accessing Text Corpora and Lexical Resources** https://www.nltk.org/book/ch02.html and **NLTK Chapter 7: Extracting Information from Text** https://www.nltk.org/book/ch07.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aggce-aN8K2T"
   },
   "source": [
    "## 1.1 Penn Treebank Corpus\n",
    "> The completion of this section prepares you to answer question 1 and 2 on **Project 1 Quiz questions**.\n",
    "\n",
    "The English Penn Treebank (PTB) corpus, and in particular the section of the corpus corresponding to the articles of *Wall Street Journal (WSJ)*, is one of the most well-known and used corpus for the evaluation of models for sequence labeling. In this project, we use this corpus for annotating each word with its *Part-of-Speech tag*.\n",
    "* A good read for this annotated corpus is **Marcinkiewicz, M. A. (1994). Building a large annotated corpus of English: The Penn Treebank. Using Large Corpora, 273.** https://dl.acm.org/doi/pdf/10.5555/972470.972475\n",
    "\n",
    "The publicly available **Penn Treebank** in NLTK library is a subset of the corpus (you can buy the entire Treebank, if you want, but you'll have to invest some $700~). This corpus was released by the University of Pennsylvania and contains 36 POS tags and 12 other tags (for punctuation and currency symbols).\n",
    "* Refer to the complete **list of POS tags for Penn-Treebank** here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "As presented in the lecture, the formalism that underlies the POS tags assigned in the Penn Treebank is the *constituency tree*, which captures *syntactic categories* (i.e., parts of speech) and relationships among words in a sentence.\n",
    "\n",
    "* This information is covered in **SLP3 17, 18**.\n",
    "\n",
    "However, in this project, we focus specifically on POS annotation at the word level, rather than sentence-level syntactic structure.\n",
    "\n",
    "**Run the code below to download and examine the Penn Treebank corpus**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "z2exon7N8SgJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
      "[('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ...]\n",
      "[('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n",
      "[('Yields', 'NNS'), ('on', 'IN'), ...]\n",
      "[('J.P.', 'NNP'), ('Bolduc', 'NNP'), (',', ','), ...]\n",
      "199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/carson.johnson/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to download and examine the Penn Treebank corpus. (Do not edit the code.)\n",
    "#This cell loads the Penn Treebank corpus from nltk and explores its structure.\n",
    "\n",
    "#No need to install nltk in google colab since it is preloaded in the environments.\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "#Ensure that the treebank corpus is downloaded\n",
    "nltk.download('treebank')\n",
    "\n",
    "#Load the treebank corpus class\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences)\n",
    "#and retrieve the word and the pre-labeled PoS tag.\n",
    "\n",
    "for fileid in treebank.fileids()[:5]:\n",
    "  print(treebank.tagged_words(fileid))\n",
    "\n",
    "print(len(treebank.fileids()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w25AstNQWU4G"
   },
   "source": [
    "In Part 2, we will see how the Hidden Markov Model is used for POS tagging over Penn Treebank Corpus.\n",
    "\n",
    "Hidden Markov Model (HMM) is a probabilistic sequence model: given a sequence of units, it computes a probability distribution over possible sequences of labels and chooses the best label sequence.\n",
    "* A good read on HMM is **Section 17.4: HMM Part-of-Speech Tagging of SLP3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ymq477gPyDI"
   },
   "source": [
    "### 1.1.1. An Alternative Annotation Scheme: Universal Dependencies\n",
    "\n",
    "An alternative format presented in the lectures is the *dependency tree* representation, which is intended to provide a logical form that captures semantic relationships. This directly contrasts with the constituency tree formalism assumed in the Penn Treebank.\n",
    "\n",
    "At the present time (as of Jan 2022), there are just over 200 treebanks of more than 100 languages available in the Universal dependencies inventory, which has *17 tags in its tagset*: https://universaldependencies.org/u/pos/, in contrast to the larger PennTreebank tag set found here: https://universaldependencies.org/tagset-conversion/en-penn-uposf.html.  The aim of such resources is to achieve cross-linguistic consistency of annotation, while still permitting language-specific extensions when necessary.\n",
    "\n",
    "Although this project uses PennTreebank Corpus, understanding the difference between the two formalisms is an essential objective of this NLP class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUUDPR3HLq95"
   },
   "source": [
    "## 1.2. CoNLL Corpus - 2002\n",
    "\n",
    "The CONLL-2002 dataset covers two languages: Spanish and Dutch. The Spanish dataset is a collection of newswire articles made available by the Spanish EFE News Agency. The articles are from May 2000. The tagged dataset contains words and entity tags only. The Dutch data consist of four editions of the Belgian newspaper \"De Morgen\" of 2000 (June 2, July 1, August 1 and September 1).\n",
    "\n",
    "* A good read is **Sang, E. F. T. K., & De Meulder, F. (1837). Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. Development, 922, 1341.** https://aclanthology.org/W02-2024.pdf. about the CoNLL2002 dataset are provided here: https://github.com/teropa/nlp/tree/master/resources/corpora/conll2002. (We note that the official name of the dataset is CONLL-2002, although the publication reference is 2003.)\n",
    "\n",
    "The files available in the CoNLL corpus include train and test data for the three parts of the CoNLL-2002 shared task:\n",
    "\n",
    "1. esp.testa: Spanish test data for the development stage\n",
    "2. esp.testb: Spanish test data\n",
    "3. esp.train: Spanish train data\n",
    "4. ned.testa: Dutch test data for the development stage\n",
    "5. ned.testb: Dutch test data\n",
    "6. ned.train: Dutch train data\n",
    "\n",
    "We narrow the scope of this project to just the Spanish portion of the CoNLL-2002 dataset. (You are free to consider testing out the Dutch dataset on your own, as a contrastive case.)  Specifically, the **two files you will access are**:\n",
    "1. For Training: 'esp.train'\n",
    "2. For Testing: 'esp.testb'\n",
    "\n",
    "We read the CoNLL corpus 2002 using *ConllCorpusReader* which is available in NLTK library. A *ConllCorpusReader* expects a data file with the following columnn types:\n",
    "\n",
    "COLUMN_TYPES = ('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore')\n",
    "where\n",
    "1. 'words': column type for words\n",
    "2. 'pos': column type for Part of Speech\n",
    "2. 'tree': column type for parse tree\n",
    "3. 'chunk': column type for short phrases present in a given structure\n",
    "4. 'ne': column type for named entities\n",
    "5. 'srl': column type for Semantic Role Labeling\n",
    "6. 'ignore': column type which can be ignored\n",
    "\n",
    "All data files contain a single word per line with an associated named entity tag in the IOB2 format. The IOB2 format (short for inside, outside, beginning) is a common tagging format for tagging tokens in a chunking task in computational linguistics. The *I-* prefix before a tag indicates that the tag is inside a chunk. An *O* tag indicates that a token belongs to no chunk. The *B-* prefix before a tag indicates that the tag is the beginning of every chunk that immediately follows another chunk without *O* tags between them.\n",
    "\n",
    "For example:\n",
    "Alex (*B-PER*)\n",
    "is (*O*)\n",
    "going (*O*)\n",
    "to (*O*)\n",
    "Los (*B-LOC*)\n",
    "Angeles (*I-LOC*)\n",
    "in (*O*)\n",
    "California (*B-LOC*).\n",
    "\n",
    "Sentence breaks are encoded as empty lines.\n",
    "\n",
    "In Part 3 we will see how the IOB2 format is used for Named Entity Recognition using the CoNLL2002 corpus.\n",
    "\n",
    "**Run the code below to download the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "XDLjHI_FoDS_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     /home/carson.johnson/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to download the dataset. (Do not edit the code.)\n",
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "nltk.download('conll2002')\n",
    "import sklearn_crfsuite\n",
    "print(nltk.corpus.conll2002.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKE1hkjfLw-9"
   },
   "source": [
    "# Part 2. Part of Speech Tagging\n",
    "Part-of-speech (POS) tagging is commonly used technology in Natural Language Processing that categorizes words of a text (corpus) in terms of specific parts of speech, depending on the definition of the word and its context.\n",
    "\n",
    "* A good read for POS tagging is **Section 17.2: Part-of-Speech Tagging of SLP3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs3yMjIkNxt0"
   },
   "source": [
    "## 2.0: Penn Treebank Corpus\n",
    "We explore the Penn Treebank corpus downloaded in Part 1.\n",
    "\n",
    "**Run the code below to read the Treebank tagged sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "09mHbCLnDyeo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAYING SENTENCES\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], [('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('of', 'IN'), ('this', 'DT'), ('British', 'JJ'), ('industrial', 'JJ'), ('conglomerate', 'NN'), ('.', '.')], [('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ('asbestos', 'NN'), ('once', 'RB'), ('used', 'VBN'), ('*', '-NONE-'), ('*', '-NONE-'), ('to', 'TO'), ('make', 'VB'), ('Kent', 'NNP'), ('cigarette', 'NN'), ('filters', 'NNS'), ('has', 'VBZ'), ('caused', 'VBN'), ('a', 'DT'), ('high', 'JJ'), ('percentage', 'NN'), ('of', 'IN'), ('cancer', 'NN'), ('deaths', 'NNS'), ('among', 'IN'), ('a', 'DT'), ('group', 'NN'), ('of', 'IN'), ('workers', 'NNS'), ('exposed', 'VBN'), ('*', '-NONE-'), ('to', 'TO'), ('it', 'PRP'), ('more', 'RBR'), ('than', 'IN'), ('30', 'CD'), ('years', 'NNS'), ('ago', 'IN'), (',', ','), ('researchers', 'NNS'), ('reported', 'VBD'), ('0', '-NONE-'), ('*T*-1', '-NONE-'), ('.', '.')], [('The', 'DT'), ('asbestos', 'NN'), ('fiber', 'NN'), (',', ','), ('crocidolite', 'NN'), (',', ','), ('is', 'VBZ'), ('unusually', 'RB'), ('resilient', 'JJ'), ('once', 'IN'), ('it', 'PRP'), ('enters', 'VBZ'), ('the', 'DT'), ('lungs', 'NNS'), (',', ','), ('with', 'IN'), ('even', 'RB'), ('brief', 'JJ'), ('exposures', 'NNS'), ('to', 'TO'), ('it', 'PRP'), ('causing', 'VBG'), ('symptoms', 'NNS'), ('that', 'WDT'), ('*T*-1', '-NONE-'), ('show', 'VBP'), ('up', 'RP'), ('decades', 'NNS'), ('later', 'JJ'), (',', ','), ('researchers', 'NNS'), ('said', 'VBD'), ('0', '-NONE-'), ('*T*-2', '-NONE-'), ('.', '.')]]\n",
      "DISPLAYING WORDS\n",
      "('Pierre', 'NNP')\n",
      "('Vinken', 'NNP')\n",
      "(',', ',')\n",
      "('61', 'CD')\n",
      "('years', 'NNS')\n",
      "('Mr.', 'NNP')\n",
      "('Vinken', 'NNP')\n",
      "('is', 'VBZ')\n",
      "('chairman', 'NN')\n",
      "('of', 'IN')\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to read the Treebank tagged sentences. (Do not edit the code.)\n",
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents())\n",
    "\n",
    "\n",
    "#print the first five sentences along with tags\n",
    "print(\"DISPLAYING SENTENCES\")\n",
    "print(nltk_data[:5])\n",
    "\n",
    "#print each word with its respective tag for first five sentences\n",
    "print(\"DISPLAYING WORDS\")\n",
    "for sent in nltk_data[:2]:\n",
    "  for tuple in sent[:5]:\n",
    "    print(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXXYO4leU_P4"
   },
   "source": [
    "## 2.1. Training and Test Samples\n",
    "> The completion of this section prepares you to answer question 3, 4, 5 and 6 on **Project 1 Quiz questions**.\n",
    "\n",
    "We now apply a 80:20 ratio to divide the training and test sets, and then compute the number of training and test samples for each set. Following this, we count the unique tags in the training data.\n",
    "\n",
    "**Run the code below to obtain training and test samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "9In3O4iwKdVN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80310\n",
      "20366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Drink', 'NN'),\n",
       " ('Carrier', 'NN'),\n",
       " ('Competes', 'VBZ'),\n",
       " ('With', 'IN'),\n",
       " ('Cartons', 'NNS')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Run this cell to obtain training and test samples.  (Do not edit the code.)\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "\n",
    "# create list of train and test tagged words\n",
    "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
    "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))\n",
    "\n",
    "# display the tagged words.\n",
    "train_tagged_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBTkRQIBSp25"
   },
   "source": [
    "**Run the code below to count unique tags present in training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "DREaKSNdKnhj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "{'VBP', 'RP', '$', 'JJS', 'PRP$', '.', 'RBR', 'CD', 'RB', 'DT', \"''\", 'NNS', 'POS', 'EX', '``', 'MD', 'WP$', 'VBG', 'VBZ', 'PDT', 'FW', 'NN', 'WP', 'NNP', '-LRB-', 'WRB', 'UH', ',', 'PRP', '#', '-RRB-', 'JJR', 'JJ', 'CC', ':', 'LS', 'VBD', 'VB', 'VBN', 'TO', '-NONE-', 'NNPS', 'RBS', 'IN', 'WDT'}\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to count unique tags present in training data.  (Do not edit the code.)\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "print(len(tags))\n",
    "print(tags)\n",
    "\n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8FzkIyjwhSD"
   },
   "source": [
    "We observe **45 tags in the training data**, including the -LRB- and -RRB- tags, which refer to \"(\" and \")\", respectively. The resulting number of tags suggests that 3 out of 48 tags are not present in the training corpus. (We will return to a comparision of this output to that of the test data in Section 2.4.) A reference for the **Penn Treebank tagset for English language** is given here: https://universaldependencies.org/tagset-conversion/en-penn-uposf.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eahxePaVnrb"
   },
   "source": [
    "## 2.2. Hidden Markov Model\n",
    "\n",
    "As mentioned before, Hidden Markov Model (HMM) is a probabilistic sequence model that leverages the principle of joint probability distribution. We calculate two probabilities for decoding algorithm *'Viterbi Algorithm'*:\n",
    "1. Emission Probability\n",
    "2. Transition Probability\n",
    "\n",
    "\n",
    "> A good read is **Section 17.4.3 The components of an HMM tagger of SLP3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgTio8-ZS4HI"
   },
   "source": [
    "### 2.2.1. Emission Probability\n",
    "\n",
    "Emission probability, also referred to as *operational likelihood*, expresses the probability that a word is generated from a tag. We first find the *tag list* and then we *count tags* to compute the emission probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q--4widnUjg"
   },
   "source": [
    "### **Programming Question #1**:\n",
    "\n",
    "**Complete the code below to define the function for computing Emission Probability. (Approximately 1 to 5 lines of code) (3 points)**\n",
    "\n",
    "<mark>After you have completed the function and executed the tests, make sure to transfer your answer to the .py file provided along with this Notebook.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "vvqSMg2BKrtN"
   },
   "outputs": [],
   "source": [
    "#@title Complete this code to define a function for computing Emission Probability.  (3 points)\n",
    "# compute Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "\n",
    "    \"\"\"\n",
    "    This function accepts word, tag and tagged words in training data to return count(w|tag) and count(tag)\n",
    "\n",
    "    :param word: string\n",
    "    :param tag: string\n",
    "    :param train_bag: list of <_word_, _tag_>\n",
    "    :return: count(w|tag) <integer>, count(tag) <integer>\n",
    "    \"\"\"\n",
    "    # Write approximately 1 to 5 lines of code:\n",
    "\n",
    "    # find the list (L) of pairs (_word_, _tag_) in train_bag having _tag_ equal to tag.\n",
    "    L = [(_word_, _tag_) for (_word_, _tag_) in train_bag if _tag_ == tag]\n",
    "    # find the count of elements in list (L) - total number of times the passed tag occurred in train_bag\n",
    "    count = len(L)\n",
    "    # find the number of times the word appears in pair <word, tag> of list (L)\n",
    "    word_count = sum(1 for (_word_, _tag_) in L if _word_ == word)\n",
    "\n",
    "\n",
    "\n",
    "    # return the word count given tag and the count of elements in the list of pairs\n",
    "    return (word_count, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "SJEWpWQnpnrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n",
      "The output must look like:\n",
      " The output for count_w_given_tag is:  1 \n",
      " The output for count_tag is:  10510\n",
      "OUTPUT:\n",
      "The output for count_w_given_tag is:  1\n",
      "The output for count_tag is:  10510\n"
     ]
    }
   ],
   "source": [
    "#@title Execute this cell to see if your changes work. (Do not change this code.)\n",
    "# Test the function. DO NOT CHANGE CODE BELOW\n",
    "try:\n",
    "    assert word_given_tag('Drink', 'NN')\n",
    "    assert word_given_tag('With', 'IN')\n",
    "    count_w_given_tag, count_tag=word_given_tag('Drink', 'NN')\n",
    "    print(\"Test Passed!\")\n",
    "    print(\"The output must look like:\\n The output for count_w_given_tag is:  1 \\n The output for count_tag is:  10510\")\n",
    "    print(\"OUTPUT:\")\n",
    "    print(\"The output for count_w_given_tag is: \", count_w_given_tag)\n",
    "    print(\"The output for count_tag is: \", count_tag)\n",
    "except AssertionError as e:\n",
    "    print(\"Test Failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LhDReofUxd6"
   },
   "source": [
    "### 2.2.2. Transition Probability\n",
    "\n",
    "We further calculate Transition Probability. HMM is based on the principle of random walk. An HMM-based solution obtains the probability of moving from one hidden state (tag-1 (t1)) to another hidden state (tag-2 (t2)) with a transition probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m4wAc4hNwEO"
   },
   "source": [
    "### **Programming Question #2**:\n",
    "\n",
    "**Complete the code below to define the function for computing Transition Probability. (Approximately 1 to 5 lines of code) (3 points)**\n",
    "\n",
    "<mark>After you have completed the function and executed the tests, make sure to transfer your answer to the .py file provided along with this Notebook.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "3Sg5hegHKuay"
   },
   "outputs": [],
   "source": [
    "#@title Complete the function below to define function for computing Transition Probability. (3 points)\n",
    "# compute  Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    \"\"\"\n",
    "    This function accepts two adjacent tags appearing in the text and tagged words in training data to return the count(t2|t1) and count(t1)\n",
    "\n",
    "    :param t1: string\n",
    "    :param t2: string\n",
    "    :param train_bag: list of <_word_, _tag_>\n",
    "    :return: count(t2|t1), count(t1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Write approximately 1 to 5 lines of code:\n",
    "\n",
    "    # find the list (T) of tags present in the pairs of train_bag <_word_, _tag_>\n",
    "    T = set([_tag_ for (_word_, _tag_) in train_bag])\n",
    "\n",
    "    # count number of times t1 is present in the List (T)\n",
    "    T1_count = sum(1 for (_word_, _tag_) in train_bag if _tag_ == t1)\n",
    "    \n",
    "    # count the number of times t2 appears after t1 in the List (T)\n",
    "    T2_given_T1_count = sum(1 for i in range(len(train_bag)-1) \n",
    "                            if train_bag[i][1] == t1 and train_bag[i+1][1] == t2)\n",
    "\n",
    "\n",
    "\n",
    "    # return count for t2 after t1, and the count for t1\n",
    "    return (T2_given_T1_count, T1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "fRNzFJ5Jsr9q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n",
      "The output must look like:\n",
      " The output for count_t2_t1 is: 468 \n",
      " The output for count_t1 is:  10510\n",
      "OUTPUT:\n",
      "The output for count_t2_t1 is:  468\n",
      "The output for count_t1 is:  10510\n"
     ]
    }
   ],
   "source": [
    "#@title Execute this cell to see if your changes work. (Do not change this code.)\n",
    "# Test the function. DO NOT CHANGE CODE BELOW\n",
    "try:\n",
    "    assert t2_given_t1('VBZ', 'NN')\n",
    "    assert t2_given_t1('NN', 'NN')\n",
    "    assert t2_given_t1('IN', 'VBZ')\n",
    "    print(\"Test Passed!\")\n",
    "    count_t2_t1, count_t1=t2_given_t1('VBZ', 'NN')\n",
    "    print(\"The output must look like:\\n The output for count_t2_t1 is: 468 \\n The output for count_t1 is:  10510\")\n",
    "    print(\"OUTPUT:\")\n",
    "    print(\"The output for count_t2_t1 is: \", count_t2_t1)\n",
    "    print(\"The output for count_t1 is: \", count_t1)\n",
    "except AssertionError as e:\n",
    "    print(\"Test Failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRclw7XGX9Ww"
   },
   "source": [
    "### 2.2.3. Transition Probability Matrix\n",
    "A transition probability matrix (tags_matrix) is used to describe the probability of a tag, given the previous tag (to its left). Such transitions are accessed by sequence labeling processes, such as POS tagging and NER, to predict a tag for a word, given that of the previous word. For example, modal verbs (MD), e.g., *will*, are very likely to be followed by a verb in the base form  (VB), e.g., *race*. Thus, we expect the probability of the sequence MD VB to be higher than, for example, MD JJ. Below we create a *t x t* matrix *(M)* where *t* is the number of tags. Matrix *M(i,j)* represents the probability of tag *j* after tag *i*. We use this tags_matrix to store transition probabilities. Transition probabilities, coupled with their corresponding emission probabilities, are used in Section 2.3 below to compute state probabilities via this equation:\n",
    "\n",
    "> *state probability = transition probability X emission probability*\n",
    "\n",
    "\n",
    "**Run the code below to obtain Matrix M(i,j).**\n",
    "> *Please note that it might take a couple of minutes to calculate the matrix.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "175EQaeYKxiN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 1.0194625e-02 9.2678407e-04 ... 0.0000000e+00\n",
      "  8.9898057e-02 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.1904762e-02 ... 0.0000000e+00\n",
      "  2.4404761e-01 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  3.3333335e-02 0.0000000e+00]\n",
      " [1.2655024e-04 1.2655024e-04 2.5689699e-02 ... 0.0000000e+00\n",
      "  1.6957732e-02 3.9230576e-03]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  5.6179776e-03 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to obtain Matrix M(i,j). (Do not edit the code.)\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)):\n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "\n",
    "print(tags_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzXeB6iIYuSk"
   },
   "source": [
    "**Run the code below to convert the matrix into a dataframe for better readability.**\n",
    "Note that the dataframe form of this table computed below contains the same data shown in the transition table above, but it is organized into a POS-tagged row/column format. The dataframe form is stored in the variable tags_df for use in Section 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "lQb92taHK0Dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VBP</th>\n",
       "      <th>RP</th>\n",
       "      <th>$</th>\n",
       "      <th>JJS</th>\n",
       "      <th>PRP$</th>\n",
       "      <th>.</th>\n",
       "      <th>RBR</th>\n",
       "      <th>CD</th>\n",
       "      <th>RB</th>\n",
       "      <th>DT</th>\n",
       "      <th>...</th>\n",
       "      <th>LS</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VB</th>\n",
       "      <th>VBN</th>\n",
       "      <th>TO</th>\n",
       "      <th>-NONE-</th>\n",
       "      <th>NNPS</th>\n",
       "      <th>RBS</th>\n",
       "      <th>IN</th>\n",
       "      <th>WDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VBP</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.145505</td>\n",
       "      <td>0.112141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162187</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.166821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089898</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJS</th>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP$</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.007739</td>\n",
       "      <td>0.045147</td>\n",
       "      <td>0.209287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.019026</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.125121</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBR</th>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203540</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.050696</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.184220</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.025705</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037487</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>0.030052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.042247</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.032230</td>\n",
       "      <td>0.069686</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064895</td>\n",
       "      <td>0.103223</td>\n",
       "      <td>0.095819</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.021777</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.001307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.024139</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.010301</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.018083</td>\n",
       "      <td>0.115732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.079566</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.019892</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.137432</td>\n",
       "      <td>0.014467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.120757</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.024069</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075087</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.020778</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.039498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.237194</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EX</th>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>``</th>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.019298</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163735</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.813261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP$</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBG</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025087</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.036332</td>\n",
       "      <td>0.184256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.045848</td>\n",
       "      <td>0.074394</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.134075</td>\n",
       "      <td>0.136417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.153981</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.191452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDT</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FW</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.102664</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.016270</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243958</td>\n",
       "      <td>0.008373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020416</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064494</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044078</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-LRB-</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRB</th>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.024233</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>0.136891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.004125</td>\n",
       "      <td>0.030678</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.080433</td>\n",
       "      <td>0.034545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>0.180566</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029007</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052937</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252357</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.018129</td>\n",
       "      <td>0.036983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032632</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-RRB-</th>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.009615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>0.011907</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>0.013172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>0.052141</td>\n",
       "      <td>0.116356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038968</td>\n",
       "      <td>0.030735</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055982</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.093541</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.131403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.022272</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.011136</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091314</td>\n",
       "      <td>0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018930</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.051029</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.123868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.098354</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022994</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.038160</td>\n",
       "      <td>0.014677</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>0.021037</td>\n",
       "      <td>0.035225</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.091487</td>\n",
       "      <td>0.014677</td>\n",
       "      <td>0.079256</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.104207</td>\n",
       "      <td>0.000978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>0.005869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015845</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>0.053991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025822</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.542840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.079812</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.074826</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>0.129350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580046</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-NONE-</th>\n",
       "      <td>0.022355</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.089805</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.023897</td>\n",
       "      <td>0.055502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030834</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>0.184621</td>\n",
       "      <td>0.075930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.142417</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNPS</th>\n",
       "      <td>0.045226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090452</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.004429</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.314984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.003923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDT</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             VBP        RP         $       JJS      PRP$         .       RBR  \\\n",
       "VBP     0.000000  0.010195  0.000927  0.000000  0.009268  0.006487  0.004634   \n",
       "RP      0.000000  0.000000  0.011905  0.000000  0.059524  0.017857  0.000000   \n",
       "$       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "JJS     0.007246  0.000000  0.014493  0.000000  0.000000  0.028986  0.000000   \n",
       "PRP$    0.000000  0.000000  0.014610  0.012987  0.000000  0.000000  0.000000   \n",
       ".       0.000000  0.000000  0.001290  0.001612  0.008062  0.000000  0.000645   \n",
       "RBR     0.008850  0.000000  0.000000  0.000000  0.000000  0.044248  0.000000   \n",
       "CD      0.003570  0.000000  0.000000  0.001071  0.000357  0.050696  0.000714   \n",
       "RB      0.030052  0.000000  0.006969  0.000000  0.000871  0.042247  0.009146   \n",
       "DT      0.001076  0.000000  0.007688  0.008610  0.000154  0.001230  0.001384   \n",
       "''      0.003617  0.000000  0.000000  0.000000  0.001808  0.000000  0.000000   \n",
       "NNS     0.092985  0.000411  0.000411  0.000000  0.000206  0.120757  0.001234   \n",
       "POS     0.000000  0.000000  0.004525  0.019608  0.000000  0.010558  0.000000   \n",
       "EX      0.216216  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "``      0.010526  0.000000  0.000000  0.000000  0.005263  0.000000  0.000000   \n",
       "MD      0.000000  0.000000  0.000000  0.001353  0.000000  0.000000  0.001353   \n",
       "WP$     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "VBG     0.000000  0.021626  0.006055  0.000000  0.025087  0.015571  0.004325   \n",
       "VBZ     0.000000  0.007026  0.006440  0.000585  0.009953  0.002927  0.003513   \n",
       "PDT     0.000000  0.000000  0.000000  0.000000  0.130435  0.000000  0.000000   \n",
       "FW      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "NN      0.003996  0.000381  0.000476  0.000190  0.000190  0.102664  0.000761   \n",
       "WP      0.000000  0.000000  0.000000  0.000000  0.010753  0.000000  0.000000   \n",
       "NNP     0.004056  0.000135  0.000135  0.000000  0.000000  0.051920  0.000000   \n",
       "-LRB-   0.000000  0.000000  0.178218  0.000000  0.000000  0.000000  0.000000   \n",
       "WRB     0.007194  0.000000  0.000000  0.000000  0.021583  0.000000  0.000000   \n",
       "UH      0.000000  0.000000  0.000000  0.000000  0.000000  0.500000  0.000000   \n",
       ",       0.008250  0.000000  0.007218  0.000773  0.002578  0.000000  0.001031   \n",
       "PRP     0.180566  0.004351  0.000000  0.000000  0.000000  0.029007  0.000725   \n",
       "#       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "-RRB-   0.019231  0.000000  0.000000  0.000000  0.000000  0.115385  0.000000   \n",
       "JJR     0.000000  0.000000  0.003236  0.000000  0.000000  0.064725  0.000000   \n",
       "JJ      0.000638  0.000213  0.001701  0.000425  0.000000  0.022964  0.000213   \n",
       "CC      0.013172  0.000000  0.020856  0.002744  0.013721  0.000000  0.002195   \n",
       ":       0.013363  0.000000  0.006682  0.000000  0.002227  0.013363  0.002227   \n",
       "LS      0.000000  0.000000  0.000000  0.000000  0.000000  0.555556  0.000000   \n",
       "VBD     0.000000  0.014815  0.014403  0.000000  0.018930  0.007407  0.002881   \n",
       "VB      0.000000  0.022994  0.008806  0.000489  0.038160  0.014677  0.007828   \n",
       "VBN     0.000000  0.009977  0.005869  0.000000  0.015845  0.008803  0.000587   \n",
       "TO      0.000000  0.000000  0.037123  0.000000  0.015661  0.000000  0.000580   \n",
       "-NONE-  0.022355  0.000964  0.002313  0.000385  0.004240  0.089805  0.000964   \n",
       "NNPS    0.045226  0.000000  0.000000  0.000000  0.000000  0.070352  0.000000   \n",
       "RBS     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "IN      0.000127  0.000127  0.025690  0.004429  0.034169  0.002531  0.000886   \n",
       "WDT     0.000000  0.000000  0.000000  0.002809  0.000000  0.000000  0.000000   \n",
       "\n",
       "              CD        RB        DT  ...        LS       VBD        VB  \\\n",
       "VBP     0.008341  0.145505  0.112141  ...  0.000000  0.000927  0.000000   \n",
       "RP      0.017857  0.035714  0.214286  ...  0.000000  0.000000  0.000000   \n",
       "$       0.987296  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "JJS     0.065217  0.043478  0.007246  ...  0.000000  0.007246  0.014493   \n",
       "PRP$    0.022727  0.001623  0.000000  ...  0.000000  0.000000  0.000000   \n",
       ".       0.007739  0.045147  0.209287  ...  0.001290  0.000645  0.000967   \n",
       "RBR     0.000000  0.123894  0.044248  ...  0.000000  0.008850  0.008850   \n",
       "CD      0.184220  0.002499  0.001428  ...  0.000000  0.006069  0.000000   \n",
       "RB      0.032230  0.069686  0.054878  ...  0.000000  0.064895  0.103223   \n",
       "DT      0.024139  0.008456  0.001230  ...  0.000000  0.001691  0.000000   \n",
       "''      0.003617  0.018083  0.115732  ...  0.001808  0.079566  0.003617   \n",
       "NNS     0.001234  0.024069  0.013166  ...  0.000000  0.075087  0.004320   \n",
       "POS     0.019608  0.003017  0.000000  ...  0.000000  0.009050  0.000000   \n",
       "EX      0.000000  0.027027  0.000000  ...  0.000000  0.135135  0.000000   \n",
       "``      0.001754  0.070175  0.168421  ...  0.000000  0.005263  0.019298   \n",
       "MD      0.000000  0.163735  0.004060  ...  0.000000  0.000000  0.813261   \n",
       "WP$     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "VBG     0.022491  0.036332  0.184256  ...  0.000000  0.001730  0.000000   \n",
       "VBZ     0.016393  0.134075  0.136417  ...  0.000000  0.001756  0.002342   \n",
       "PDT     0.000000  0.000000  0.869565  ...  0.000000  0.000000  0.000000   \n",
       "FW      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "NN      0.005043  0.016270  0.005423  ...  0.000000  0.048240  0.001618   \n",
       "WP      0.005376  0.021505  0.043011  ...  0.000000  0.000000  0.000000   \n",
       "NNP     0.020416  0.006220  0.002163  ...  0.000000  0.064494  0.000811   \n",
       "-LRB-   0.079208  0.009901  0.079208  ...  0.000000  0.000000  0.000000   \n",
       "WRB     0.021583  0.028777  0.330935  ...  0.000000  0.014388  0.000000   \n",
       "UH      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       ",       0.024233  0.054395  0.136891  ...  0.000000  0.054911  0.002836   \n",
       "PRP     0.000000  0.052937  0.008702  ...  0.000000  0.252357  0.007977   \n",
       "#       1.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "-RRB-   0.009615  0.048077  0.057692  ...  0.000000  0.057692  0.000000   \n",
       "JJR     0.003236  0.003236  0.019417  ...  0.000000  0.000000  0.000000   \n",
       "JJ      0.021688  0.002977  0.004253  ...  0.000000  0.000851  0.000000   \n",
       "CC      0.040615  0.052141  0.116356  ...  0.000000  0.038968  0.030735   \n",
       ":       0.093541  0.033408  0.131403  ...  0.008909  0.022272  0.006682   \n",
       "LS      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "VBD     0.051029  0.077778  0.123868  ...  0.000000  0.000000  0.002058   \n",
       "VB      0.021037  0.035225  0.232877  ...  0.000000  0.000489  0.000978   \n",
       "VBN     0.010563  0.022887  0.053991  ...  0.000000  0.000587  0.000000   \n",
       "TO      0.074826  0.007541  0.129350  ...  0.000000  0.000000  0.580046   \n",
       "-NONE-  0.003083  0.023897  0.055502  ...  0.000000  0.030834  0.009828   \n",
       "NNPS    0.000000  0.010050  0.005025  ...  0.000000  0.035176  0.000000   \n",
       "RBS     0.000000  0.233333  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "IN      0.063275  0.012528  0.314984  ...  0.000000  0.000759  0.000000   \n",
       "WDT     0.005618  0.005618  0.028090  ...  0.000000  0.005618  0.000000   \n",
       "\n",
       "             VBN        TO    -NONE-      NNPS       RBS        IN       WDT  \n",
       "VBP     0.162187  0.010195  0.166821  0.000000  0.000000  0.089898  0.000000  \n",
       "RP      0.000000  0.017857  0.119048  0.000000  0.000000  0.244048  0.000000  \n",
       "$       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "JJS     0.007246  0.000000  0.000000  0.007246  0.000000  0.144928  0.000000  \n",
       "PRP$    0.008117  0.000000  0.000000  0.001623  0.001623  0.001623  0.000000  \n",
       ".       0.001612  0.001612  0.019026  0.002580  0.000322  0.125121  0.000645  \n",
       "RBR     0.035398  0.008850  0.026549  0.000000  0.000000  0.203540  0.000000  \n",
       "CD      0.003570  0.025705  0.202428  0.000000  0.000000  0.037487  0.002142  \n",
       "RB      0.095819  0.016115  0.021777  0.000436  0.000000  0.121951  0.001307  \n",
       "DT      0.011531  0.000308  0.001999  0.003536  0.002460  0.010301  0.000461  \n",
       "''      0.009042  0.003617  0.019892  0.001808  0.001808  0.137432  0.014467  \n",
       "NNS     0.020778  0.017692  0.039498  0.000000  0.000206  0.237194  0.014400  \n",
       "POS     0.015083  0.000000  0.000000  0.004525  0.003017  0.003017  0.000000  \n",
       "EX      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "``      0.015789  0.000000  0.033333  0.000000  0.000000  0.061404  0.000000  \n",
       "MD      0.000000  0.000000  0.005413  0.000000  0.000000  0.000000  0.000000  \n",
       "WP$     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "VBG     0.020761  0.045848  0.074394  0.000865  0.000000  0.125433  0.000000  \n",
       "VBZ     0.153981  0.008197  0.191452  0.000000  0.000000  0.081967  0.000000  \n",
       "PDT     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "FW      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "NN      0.008278  0.021218  0.040438  0.000000  0.000000  0.243958  0.008373  \n",
       "WP      0.000000  0.000000  0.768817  0.000000  0.005376  0.016129  0.000000  \n",
       "NNP     0.000946  0.004056  0.005679  0.017036  0.000000  0.044078  0.000541  \n",
       "-LRB-   0.029703  0.000000  0.009901  0.000000  0.000000  0.059406  0.000000  \n",
       "WRB     0.000000  0.000000  0.043165  0.000000  0.007194  0.028777  0.000000  \n",
       "UH      0.000000  0.000000  0.000000  0.000000  0.000000  0.500000  0.000000  \n",
       ",       0.022686  0.004125  0.030678  0.000516  0.000516  0.080433  0.034545  \n",
       "PRP     0.001450  0.018129  0.036983  0.000000  0.000000  0.032632  0.000000  \n",
       "#       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "-RRB-   0.000000  0.019231  0.038462  0.000000  0.000000  0.076923  0.009615  \n",
       "JJR     0.003236  0.006472  0.019417  0.000000  0.000000  0.349515  0.000000  \n",
       "JJ      0.003402  0.011907  0.021688  0.001063  0.000000  0.061025  0.000000  \n",
       "CC      0.017014  0.004391  0.009330  0.002744  0.000000  0.055982  0.001098  \n",
       ":       0.011136  0.002227  0.033408  0.000000  0.000000  0.091314  0.004454  \n",
       "LS      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "VBD     0.098354  0.022222  0.271605  0.000000  0.000000  0.111934  0.000000  \n",
       "VB      0.091487  0.014677  0.079256  0.000978  0.000978  0.104207  0.000978  \n",
       "VBN     0.025822  0.013498  0.542840  0.000000  0.000587  0.079812  0.000000  \n",
       "TO      0.001740  0.000000  0.006381  0.000000  0.000000  0.004060  0.000000  \n",
       "-NONE-  0.010214  0.184621  0.075930  0.000000  0.000193  0.142417  0.000000  \n",
       "NNPS    0.000000  0.005025  0.010050  0.010050  0.000000  0.090452  0.000000  \n",
       "RBS     0.033333  0.000000  0.000000  0.000000  0.000000  0.033333  0.000000  \n",
       "IN      0.003797  0.001139  0.034548  0.002278  0.000000  0.016958  0.003923  \n",
       "WDT     0.000000  0.000000  0.842697  0.000000  0.000000  0.005618  0.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Run this cell to convert the matrix into a dataframe for better readability. (Do not edit the code.)\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "display(tags_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B09vb-OfZNPG"
   },
   "source": [
    "## 2.3. Viterbi Algorithm\n",
    "\n",
    "The Viterbi Algorithm is a dynamic programming decoder for HMMs.\n",
    "* A good read for Viterbi Algorithm is **Section 17.4.5: The Viterbi Algorithm in Chapter 17: Sequence Labeling for Parts of Speech and Named Entities in SLP3**.\n",
    "\n",
    "In this section we will first define a function to compute a list of state probabilities (based on the matrix computed above), and then will define the Viterbi algorithm to use these state probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uapr71XfsVja"
   },
   "source": [
    "### **Programming Question #3**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTlkojkmUP3s"
   },
   "source": [
    "**Complete this function to produce a list of probabilities of different states for each tag. (Approximately 1 to 5 lines of code) (3 points)**\n",
    "\n",
    "<mark>After you have completed the function and executed the tests, make sure to transfer your answer to the .py file provided along with this Notebook.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "rZTov4fUPWCP"
   },
   "outputs": [],
   "source": [
    "#@title Edit this code to produce a list of probabilities of different states for each tag. (3 points)\n",
    "def compute_state_probability(key, word, T, state):\n",
    "    \"\"\"\n",
    "    This function accepts key, word, list of tags T, the previous state (tag) and returns the list of probabilities of each tag in T\n",
    "    being the next state\n",
    "\n",
    "\n",
    "    :param key: int the position of the word in the sentence\n",
    "    :param word: string\n",
    "    :param T: List of unique tags\n",
    "    :param tag: string\n",
    "    :param p: List of probabilities for current iteration\n",
    "\n",
    "    :return: List<state_probabilities>\n",
    "    \"\"\"\n",
    "    p = []\n",
    "    for tag in T:\n",
    "        if key == 0:\n",
    "            transition_p = tags_df.loc['.', tag]\n",
    "        else:\n",
    "            transition_p = tags_df.loc[state[-1], tag]\n",
    "\n",
    "        # Write approximately 1 to 5 lines of code to compute emission and state probabilities:\n",
    "\n",
    "        # calculate Emission probabilities\n",
    "        word_count, tag_count = word_given_tag(word, tag)\n",
    "        emission_p = word_count / tag_count if tag_count > 0 else 0\n",
    "\n",
    "        # calculate state probabilities\n",
    "        state_probability = transition_p * emission_p\n",
    "\n",
    "        # add state probability in the list\n",
    "        p.append(state_probability)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "K73mqCPnsCSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n",
      "The output must look like:\n",
      " The output for p is: [0, 0, 0, ..., 0.0003800179891361825,..., 0, 0, 0]\n",
      "OUTPUT:\n",
      "The output for p is:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003800179891361825, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#@title Execute this cell to see if your changes work. (Do not change this code.)\n",
    "# Test the function. DO NOT CHANGE CODE BELOW\n",
    "try:\n",
    "    p=[]\n",
    "    state=[]\n",
    "    T = list(set([pair[1] for pair in train_tagged_words]))\n",
    "    assert compute_state_probability( 0, 'At', T, state)\n",
    "    p=  compute_state_probability( 0, 'At', T, state)\n",
    "    print(\"Test Passed!\")\n",
    "    print(\"The output must look like:\\n The output for p is: [0, 0, 0, ..., 0.0003800179891361825,..., 0, 0, 0]\")\n",
    "    print(\"OUTPUT:\")\n",
    "    print(\"The output for p is: \", p)\n",
    "except AssertionError as e:\n",
    "    print(\"Test Failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-XHb6zUUIvu"
   },
   "source": [
    "**Execute this code to define the function for Viterbi algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "tr7_hdG4K2oi"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to define the function for Viterbi Algorithm. (Do not edit the code.)\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "\n",
    "        p=compute_state_probability(key, word, T, state)\n",
    "        pmax = max(p)\n",
    "\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)]\n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpDfosTwaECM"
   },
   "source": [
    "**Execute this code to test the Viterbi algorithm on a few sample sentences of test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "cQkfoAHnK5Mp"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to test the Viterbi algorithm. (Do not edit the code.)\n",
    "#Once complete, test_subset_base contains the human-labeled\n",
    "#tags corresponding toe the words in test_tagged_words.\n",
    "random.seed(1234)      #define a random seed to get same sentences when run multiple times\n",
    "\n",
    "# choose 10 random numbers\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(10)]\n",
    "\n",
    "# list of 10 sents on which we test the model\n",
    "test_subset = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_subset_base = [tup for sent in test_subset for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_subset_words = [tup[0] for sent in test_subset for tup in sent]\n",
    "\n",
    "# list of tags in test subset\n",
    "test_subset_tags = [tup[1] for sent in test_subset for tup in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC3DAQBhaURA"
   },
   "source": [
    "## 2.4. Viterbi Evaluation for POS Tagging\n",
    "> The completion of this section prepares you to answer question 7 and 8 on **Project 1 Quiz questions**.\n",
    "\n",
    "The Viterbi algorithm first sets up a lattice with one column for each word, and rows for tags where each cell represents each tag. Each cell $\\mathrm{t_i}$ of the lattice represents the probability of that the HMM is in a given state after seeing the the previous observations (words: $\\mathrm{w_1, w_2, ..., w_{i-1}}$) and passing through the most probable cell sequence $\\mathrm{t_1, t_2, ..., t_{i-1}}$. The value of each cell is computed by recursively taking the most probable path (the maximum over all possible previous state sequences) that could lead us to this cell.\n",
    "\n",
    "* A good read is **17.4.5 The Viterbi Algorithm in SLP3**.\n",
    "\n",
    "One way to evaluate the accuracy of sequence labeling problems is to compare the output tags (tagged_seq) for an input sequence (test_subset_words) to a human-labeled sequence that we already know to be correct (test_subset_base). Here, we will verify only 10 sentences to check the accuracy as verification of the entire test set takes a very long time. Technically this notion of \"accuracy\" is referred to as \"precision\" (how many answers are correct out of the total number of answers), which we will learn more about in Project 2 and in the Evaluation module in the final segment of this course. (We will refer to it as \"precision\" further below.)\n",
    "\n",
    "**Execute this code to test 10 sentences to find accuracy and time taken.**\n",
    "\n",
    "> *Please note that it might take a couple of minutes to execute the Viterbi algorithm.*\n",
    "\n",
    "> There could be some variability in Viterbi accuracy with multiple runs, but this does not adversely impact scoring for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "PpcG2OpmK7v1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  41.03318548202515\n",
      "Viterbi Algorithm Accuracy:  90.9090909090909\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to test 10 sentences. (Do not edit this code.)\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_subset_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_subset_base) if i == j]\n",
    "\n",
    "\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Viterbi Algorithm Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOjW8_hdcCVF"
   },
   "source": [
    "The Viterbi algorithm accuracy was computed above on 10 randomly chosen sentences. We exploit the resulting tagged sequence and test_subset_base to obtain more refined classification results, in terms of precision, recall, F1, and support. We will learn more about precision, recall, and F1 in Project 2. Below, the term *support* refers to the number of instances per tag. Note: The notions of micro/macro averaging will be discussed in more detail in an evaluation module later in the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "yIfNjq6daPeu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ''       1.00      1.00      1.00         3\n",
      "           ,       1.00      1.00      1.00        13\n",
      "      -NONE-       1.00      0.93      0.97        15\n",
      "           .       1.00      1.00      1.00        10\n",
      "          CC       1.00      1.00      1.00         4\n",
      "          CD       1.00      1.00      1.00         2\n",
      "          DT       1.00      1.00      1.00        17\n",
      "          IN       1.00      1.00      1.00        16\n",
      "          JJ       0.86      0.86      0.86         7\n",
      "         JJR       0.00      0.00      0.00         1\n",
      "          MD       1.00      1.00      1.00         2\n",
      "          NN       1.00      0.90      0.95        29\n",
      "         NNP       1.00      0.87      0.93        23\n",
      "        NNPS       1.00      1.00      1.00         1\n",
      "         NNS       1.00      0.89      0.94         9\n",
      "         PRP       1.00      1.00      1.00         4\n",
      "          RB       1.00      0.91      0.95        11\n",
      "         RBR       0.00      0.00      0.00         0\n",
      "          TO       1.00      1.00      1.00         4\n",
      "          VB       0.80      0.67      0.73         6\n",
      "         VBD       0.75      0.60      0.67         5\n",
      "         VBG       1.00      0.86      0.92         7\n",
      "         VBN       0.57      0.67      0.62         6\n",
      "         VBP       0.20      0.75      0.32         4\n",
      "         VBZ       1.00      1.00      1.00         5\n",
      "         WDT       1.00      1.00      1.00         1\n",
      "          WP       1.00      1.00      1.00         1\n",
      "          ``       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.91       209\n",
      "   macro avg       0.86      0.85      0.85       209\n",
      "weighted avg       0.95      0.91      0.93       209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/cap4641/share/conda/envs/cap4641/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/blue/cap4641/share/conda/envs/cap4641/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/blue/cap4641/share/conda/envs/cap4641/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/blue/cap4641/share/conda/envs/cap4641/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/blue/cap4641/share/conda/envs/cap4641/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/blue/cap4641/share/conda/envs/cap4641/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to calculate the inventory of POS tags in the test corpus and the results of POS tagging on this corpus. (Do not edit this code.)\n",
    "from sklearn.metrics import classification_report\n",
    "actual_labels=[j for i, j in test_subset_base]\n",
    "predict_labels=[j for i,j in tagged_seq]\n",
    "#lab=['IN', 'JJ', 'NN', 'VB', 'RB', 'DT', 'CC' ] # We analyze some well-known POS tags.\n",
    "\n",
    "results=classification_report(actual_labels, predict_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN7zK-LMcWpR"
   },
   "source": [
    "## 2.5. For further study\n",
    "\n",
    "You are free to acquire results and play with different combinations of POS tags for better inference. With these results, we leave open questions for you:\n",
    "1. Observe the worst performing and the best performing POS tags. Analyze how the worst performing POS tags can be improved.\n",
    "2. Explore the variation in the number of instances for each POS tag and how it affects the results.\n",
    "3. Investigate whether a rule-based tagger, in-place of a probabilistic tagger, help in reducing the computational time (thereby, maintaining/ improving results).\n",
    "4. Consider what it would take to explore the HMM-Viterbi algorithm for POS tagging for a different language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN-gptggogUG"
   },
   "source": [
    "# Part 3. Named Entity Recognition\n",
    "\n",
    "Named-entity recognition (NER), also known as (named) entity identification, entity chunking, and entity extraction, is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as person names, organizations, locations.\n",
    "* A good read is **Introduction to Chapter 17: Sequence Labeling for Parts of\n",
    "Speech and Named Entities in SLP3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzqIx2jiRX3Q"
   },
   "source": [
    "## 3.0 CoNLL2002 dataset\n",
    "\n",
    "We will use the same CoNLL2002 corpus for the NER task that was downloaded in Part 1.2. We further analyze this dataset. There are as many as 6 fileids available in the dataset. In each file, every line represents a tagged word seperated by an empty space as *'word pos_tag NER_tag'*. Thus, three columns in every line consists of\n",
    "\n",
    "1. The first column: Word\n",
    "2. The second column: POS tag\n",
    "3. The third column: Named entity\n",
    "\n",
    "The data are represented with BIO tagging.\n",
    "* A good read on BIO tagging is **Figure 17.3: Named Entities and Named Entity Tagging, showing IO, BIO, and BIOES taggings in Chapter 17: Sequence Labeling for Parts of Speech and Named Entities of SLP3**. For example, BIOES is an acronym that refers to:\n",
    "* B: labels a token that begins a span\n",
    "* I: labels a token inside a span\n",
    "* O: labels a token outside of any span\n",
    "* E: labels a token that ends a span and\n",
    "* S: labels a single-token span\n",
    "\n",
    "In this programming question, we focus on the simpler BIO tagging scheme: any token that begins a span of interest is labeled B, tokens that occur inside a span are labeled I, and any tokens outside of any span of interest are labeled O. We also augment the labels with named-entity tags:\n",
    "\n",
    "1. 'B-LOC' : beginning location\n",
    "2. 'B-MISC': beginning miscellaneous\n",
    "3. 'B-ORG' : beginning organization\n",
    "4. 'B-PER' : beginning person\n",
    "5. 'I-LOC' : inside location\n",
    "6. 'I-MISC': inside miscellaneous\n",
    "7. 'I-ORG' : inside organization\n",
    "8. 'I-PER' : inside person\n",
    "9. 'O'.    : Outside of span of interest\n",
    "\n",
    "**Run the code below to read the test file.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "efUpfDjYJRJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sao NC B-LOC\n",
      "Paulo VMI I-LOC\n",
      "( Fpa O\n",
      "Brasil NC B-LOC\n",
      ") Fpt O\n",
      ", Fc O\n",
      "23 Z O\n",
      "may NC O\n",
      "( Fpa O\n",
      "EFECOM N\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to read the *test* file. (Do not edit this code.)\n",
    "print(nltk.corpus.conll2002.raw('esp.testa')[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrjJGFdq8jp-"
   },
   "source": [
    "## 3.1 Training and Test Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TijF_9RMSzRC"
   },
   "source": [
    "We train and test a NER model using Spanish data from the CoNLL2002 dataset. The model is trained on *'esp.train'* and tested on *'esp.testb'*. We extract the dataset into two variables, *train_sents* and *test_sents*, respectively. The *iob_sents* function extracts the raw text of file in the form of a list of *tuples <word, pos_tag, ner_tag>* for each sentence.\n",
    "\n",
    "We further explore the first annotated sentence in the training dataset to examine how POS tags and NE tags are assigned.  \n",
    "\n",
    "**Execute the code below to obtain training and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "ftr5HiOCoLCt",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Melbourne', 'NP', 'B-LOC'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('Australia', 'NP', 'B-LOC'),\n",
       " (')', 'Fpt', 'O'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('25', 'Z', 'O'),\n",
       " ('may', 'NC', 'O'),\n",
       " ('(', 'Fpa', 'O'),\n",
       " ('EFE', 'NC', 'B-ORG'),\n",
       " (')', 'Fpt', 'O'),\n",
       " ('.', 'Fp', 'O')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Run this cell to obtain training and test data for NER task. (Do not edit the code.)\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "train_sents[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2_JnqlrTX-z"
   },
   "source": [
    "### **Programming Question #4**:\n",
    "\n",
    "To understand the nature of the dataset, we now check the distribution of NER tags in the dataset for both training and test data.\n",
    "\n",
    "**Complete this code to find the NER tag distribution in dataset. (Approximately 2 to 10 lines of code) (3 points)**\n",
    "\n",
    "<mark>After you have completed the function and executed the tests, make sure to transfer your answer to the .py file provided along with this Notebook.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "x272gvXeQTDk"
   },
   "outputs": [],
   "source": [
    "#@title Complete this code to find the NER tag distribution in dataset. (3 points)\n",
    "\n",
    "def compute_tag_distribution(sents):\n",
    "    \"\"\"\n",
    "    This function accepts a tuple <word, pos, ner> to return the NER_tag frequency distribution\n",
    "\n",
    "    :param List of list of tuples <word, pos, ner>: list of list of (string, string, string)\n",
    "    :return ner_tag_frequency: a dictionary of frequency of NER tags.\n",
    "    \"\"\"\n",
    "    ner_tag_frequency=dict()\n",
    "\n",
    "    # Write approximate 2-10 lines of code to create dictionay of <word, NER_tag> pairs.\n",
    "    # In a list of sents given as a tuple <word, pos, ner>, count how many words\n",
    "    # are there for each ner tag:\n",
    "    \n",
    "    for sent in sents:\n",
    "        for (word, pos, ner) in sent:\n",
    "            ner_tag_frequency[ner] = ner_tag_frequency.get(ner, 0) + 1\n",
    "\n",
    "    return ner_tag_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "-Fx2PhMkREmn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n",
      "The output for training corpora must look like: {'B-LOC': 4913, 'O': 231920, 'B-ORG': 7390, 'B-PER': 4321, ...} \n",
      "OUTPUT for train_sents ner_tag_frequency is: {'B-LOC': 4913, 'O': 231920, 'B-ORG': 7390, 'B-PER': 4321, 'I-PER': 3903, 'B-MISC': 2173, 'I-ORG': 4992, 'I-LOC': 1891, 'I-MISC': 3212}\n",
      "The output for test corpora must look like: {'B-LOC': 1084, 'I-LOC': 325, 'O': 45355, 'B-ORG': 1400, ...} \n",
      "OUTPUT for test_sents ner_tag_frequency is: {'B-LOC': 1084, 'I-LOC': 325, 'O': 45355, 'B-ORG': 1400, 'B-MISC': 339, 'B-PER': 735, 'I-PER': 634, 'I-ORG': 1104, 'I-MISC': 557}\n"
     ]
    }
   ],
   "source": [
    "#@title Execute this cell to see if your changes work (Do not change this code.)\n",
    "# Test the function. DO NOT CHANGE CODE BELOW\n",
    "try:\n",
    "    assert compute_tag_distribution(train_sents)\n",
    "    assert compute_tag_distribution(test_sents)\n",
    "    train_ner_tag_frequency=compute_tag_distribution(train_sents)\n",
    "    test_ner_tag_frequency=compute_tag_distribution(test_sents)\n",
    "    print(\"Test Passed!\")\n",
    "    print(\"The output for training corpora must look like: {'B-LOC': 4913, 'O': 231920, 'B-ORG': 7390, 'B-PER': 4321, ...} \")\n",
    "    print(\"OUTPUT for train_sents ner_tag_frequency is:\", train_ner_tag_frequency)\n",
    "    print(\"The output for test corpora must look like: {'B-LOC': 1084, 'I-LOC': 325, 'O': 45355, 'B-ORG': 1400, ...} \")\n",
    "    print(\"OUTPUT for test_sents ner_tag_frequency is:\", test_ner_tag_frequency)\n",
    "except AssertionError as e:\n",
    "    print(\"Test Failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JadtErrT0CZ"
   },
   "source": [
    "We observe that the NER tag distribution differs due to the size difference between the training and test corpora. However, the distributions follow a similar trend, with the highest number of tags being \"O\", etc. in both distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVxtE22gonFs",
    "tags": []
   },
   "source": [
    "## 3.2. Feature Extraction\n",
    "> The completion of this section prepares you to answer question 9 and 10 on **Project 1 Quiz questions**.\n",
    "\n",
    "As a starting point for the named-entity recognition task, we extract features for every word. A feature is an individual measurable property or characteristic of a word, for example, its part of speech (e.g., the POS for \"eat\" is a Verb). Choosing informative, discriminating and independent features is crucial for development of effective algorithms in pattern recognition, classification and regression. Producing accurate output for sequence labeling relies crucially on careful selection of features.\n",
    "\n",
    "In addition to POS tags, other types of features may be extracted, e.g., word parts, simplified POS tags, lower/title/upper case flags, and features of nearby words.  To employ all such features in the task of Named Entity recognition, we start by converting them into sklearn-crfsuite format. Each sentence is converted into a list of dicts, i.e., a dictionary that maps feature names to feature indices. This is a very simple baseline; you certainly can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_KUtpOLl20k"
   },
   "source": [
    "We add following feature extraction steps for a given word and its neighbors (steps 1, 2, 3, and 4 refer to boolean values and 5 refers to a multi-category POS):\n",
    "1. Word is lower-case (non-capitalized letters, e.g., \"a\")\n",
    "2. Case of a word is `upper' (capitalized letters, e.g., \"A\")\n",
    "3. Word is title (e.g., \"Mr.\")\n",
    "4. Word is a digit (e.g., \"10\")\n",
    "5. POS tag of word (e.g., \"JJ\")\n",
    "6. Repeat steps from 2 to 5 for its neighbors\n",
    "\n",
    "Tag frequency may influence the feature vector. Thus, features need to be weighted according to the number of times their corresponding tag appears. This introduces 'bias', an independent feature, which balances out the influence of varying tag frequency in the training corpus. Using 'bias' is a prevailing approach to address *feature bias* in adjusting the training loss for manually designed biased features. However, for this programming question, we make the simplifying assumption of uniform NER tag frequency distribution during model, e.g., ORG is assumed to occur as frequently as PERS. You are free to experiment independently with a weighted distribution for the 'bias' feature to train the model, but please retain a 'bias' setting of 1.0 for your final project submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBfr8tH0lERL"
   },
   "source": [
    "### **Programming Question #5**\n",
    "\n",
    "You will create a new function to define additional features for input words. Specifically, we add some basic NLP features like ‘postag’ and various features relating to letter case (lower, upper, etc.). Other than the given features, we can use any other meaningful handcrafted rule as additional feature, for example, position of a word.\n",
    "\n",
    "**Complete the code below to define function for additional features. (Approximately 2 to 10 lines of code) (1 point)**\n",
    "\n",
    "<mark>After you have completed the function and executed the tests, make sure to transfer your answer to the .py file provided along with this Notebook.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "7eNhw5pTU2iv"
   },
   "outputs": [],
   "source": [
    "#@title Complete the code to define function for additional features. (1 point)\n",
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    This function accepts a sent (list of tuple in sentence) to return the additional features.\n",
    "\n",
    "    :param sent: List of (string, string, string)\n",
    "    :return features: dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "\n",
    "\n",
    "        #find (i) previous word and (ii) postag for previous word.\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "\n",
    "        # Add the following features for the previous word:\n",
    "        # lowercased version of the word\n",
    "        # value indicating whether word is title\n",
    "        # value indicating whether word is uppercase\n",
    "        # POS tag of the word\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "\n",
    "        # Write approximately 8-10 lines of code to add features for the next word.\n",
    "        #find (i) next word and (ii) postag for next word.\n",
    "        word2 = sent[i+1][0]\n",
    "        postag2 = sent[i+1][1]\n",
    "\n",
    "        # Add the following features for the next word:\n",
    "        # lowercased version of the word\n",
    "        # value indicating whether word is title\n",
    "        # value indicating whether word is uppercase\n",
    "        # POS tag of the word\n",
    "        features.update({\n",
    "            '+1:word.lower()': word2.lower(),\n",
    "            '+1:word.istitle()': word2.istitle(),\n",
    "            '+1:word.isupper()': word2.isupper(),\n",
    "            '+1:postag': postag2,\n",
    "            '+1:postag[:2]': postag2[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "ees_fR0RjG2F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n",
      "The output must look like: {'bias': 1.0, 'word.lower()': 'del', 'word[-3:]': 'del', ..., }\n",
      "OUTPUT:\n",
      "{'bias': 1.0, 'word.lower()': 'del', 'word[-3:]': 'del', 'word[-2:]': 'el', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'SP', 'postag[:2]': 'SP', '-1:word.lower()': 'petición', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NC', '-1:postag[:2]': 'NC', '+1:word.lower()': 'abogado', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'NC', '+1:postag[:2]': 'NC'}\n"
     ]
    }
   ],
   "source": [
    "#@title Execute this cell to see if your changes work (Do not change this code.)\n",
    "# Test the function. DO NOT CHANGE CODE BELOW\n",
    "try:\n",
    "    assert word2features(train_sents[3], 2)\n",
    "    features=word2features(train_sents[3], 2)\n",
    "    print(\"Test Passed!\")\n",
    "    print(\"The output must look like: {'bias': 1.0, 'word.lower()': 'del', 'word[-3:]': 'del', ..., }\")\n",
    "    print(\"OUTPUT:\")\n",
    "    print(features)\n",
    "except AssertionError as e:\n",
    "    print(\"Test Failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "bzoXo6ozojvb"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to define additional functions for feature extraction, to be used below. (Do not edit the code.)\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8mIGgJSmXHC"
   },
   "source": [
    "### Extracting features\n",
    "\n",
    "The input to feature extraction is a sentence. We obtain characteristics for each word in a sentence as a set of *features* in the form of a dictionary, as illustrated in Part 3.1. We now use sent2feature and sent2labels for feature extraction at the sentence level, recursively extracting features at the word level using the word2feature function.\n",
    "\n",
    "**Execute the code below to extract features in the accepted format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "4qQomxoH1ztZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': ')',\n",
       " 'word[-3:]': ')',\n",
       " 'word[-2:]': ')',\n",
       " 'word.isupper()': False,\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " 'postag': 'Fpt',\n",
       " 'postag[:2]': 'Fp',\n",
       " '-1:word.lower()': 'australia',\n",
       " '-1:word.istitle()': True,\n",
       " '-1:word.isupper()': False,\n",
       " '-1:postag': 'NP',\n",
       " '-1:postag[:2]': 'NP',\n",
       " '+1:word.lower()': ',',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'Fc',\n",
       " '+1:postag[:2]': 'Fc'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Run this cell to extract features in the accepted format. (Do not edit the code.)\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "X_train[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk2XkFQymeqW"
   },
   "source": [
    "## 3.3. Conditional Random Field (CRF) model\n",
    "\n",
    "A Conditional Random Field (CRF) is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. CRF is a labeler in which the tag of the present word (denoted as yᵢ) depends only on the tag of just the previous word(denoted by yᵢ₋₁).\n",
    "* A more detailed description of CRFs is provided at [Medium platform](https://medium.com/data-science-in-your-pocket/named-entity-recognition-ner-using-conditional-random-fields-in-nlp-3660df22e95c).\n",
    "* Another good read on CRF is **Section 17.5: Conditional Random Fields of SLP3**.\n",
    "\n",
    "Researchers often adopt CRFs over HMMs for Named Entity Recognition, given their incorporation of conditional probabilities, in contrast to the joint distributions used in HMMs. Named entity recognition is a computationally complex problem and modeling joint probabilities has disadvantages due to computational complexity. We note that it is also possible to apply CRFs to the problem of POS tagging, and we encourage you to try this on your own.\n",
    "\n",
    " * Additional information about CRF is available in **sklearn-crfsuite**: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite\n",
    "\n",
    "**Execute the code below to train the model.**\n",
    "\n",
    "> *Please note that it might take a couple of minutes for this calculation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "Tn0ztc5FpCDP"
   },
   "outputs": [],
   "source": [
    "#@title Run this code to train the model. (Do not edit the code.)\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=1.0,\n",
    "    c2=1e-3,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=True,\n",
    ")\n",
    "crf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyNOj9PspG_0"
   },
   "source": [
    "## 3.4. CRF Evaluation for Named Entity Recognition\n",
    "> The completion of this section prepares you to answer question 11 and 12 on **Project 1 Quiz questions**.\n",
    "\n",
    "We further evaluate our model using sklearn metrics.\n",
    "\n",
    "**Run the code below to test the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "-oKoOlPxpHV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.67      0.61      0.64      1084\n",
      "      B-MISC       0.35      0.10      0.15       339\n",
      "       B-ORG       0.66      0.76      0.71      1400\n",
      "       B-PER       0.74      0.77      0.75       735\n",
      "       I-LOC       0.49      0.24      0.32       325\n",
      "      I-MISC       0.55      0.20      0.30       557\n",
      "       I-ORG       0.63      0.79      0.70      1104\n",
      "       I-PER       0.81      0.89      0.85       634\n",
      "           O       0.99      1.00      0.99     45355\n",
      "\n",
      "    accuracy                           0.95     51533\n",
      "   macro avg       0.66      0.59      0.60     51533\n",
      "weighted avg       0.95      0.95      0.95     51533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to test the model. (Do not edit the code.)\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_pred=crf.predict(X_test)\n",
    "\n",
    "result=flat_classification_report(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmjBhTzct6hH"
   },
   "source": [
    "If computed correctly, your output above indicates an unusually high value for the 'O' (outside) prediction. More on this point in the question below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVeCLYNHkIB7"
   },
   "source": [
    "## 3.5. For further study\n",
    "\n",
    "With these observations, we give the outlook of this experiment and open problems in this area:\n",
    "1. Consider the exploration of hybrid CRF approaches with other sequence-to-sequence models.\n",
    "2. Examine the low scores for I-MISC and B-MISC, and possible ways to improve them.\n",
    "3. Obtain results without 'O' NER tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0l4D7LTqw2W"
   },
   "source": [
    "# Part 4: Uploading responses for Grading\n",
    "\n",
    "To upload your responses for grading, please follow the instructions provided in the `Project 1 - Sequence Labeling` page in Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lucqqa3dIbr_"
   ],
   "provenance": [
    {
     "file_id": "1ehhHZAQK7yKk01e_bJiN7aqrQDUuqAe5",
     "timestamp": 1696259984762
    },
    {
     "file_id": "1fqlzQ81HahjmdWpsJohuLSA6u3pTde5E",
     "timestamp": 1664550598362
    }
   ]
  },
  "kernelspec": {
   "display_name": "CAP4641_PROJECTS",
   "language": "python",
   "name": "cap4641_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
